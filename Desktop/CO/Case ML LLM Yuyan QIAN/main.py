import sqlite3
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import os
import openai 

model = SentenceTransformer( "paraphrase-xlm-r-multilingual-v1").to('cpu')

def get_TC(file,column1, column2=None, indexURL = None):
    """
    A function that returns the specified columns stored in the database.

    Args : 
        fils: str - the name of the database file
        column1: str - the name of first column needed
        column2: str - the name of the second column, defaults to None
        indexURI: int - the URI of the row needed, defaults to None

    return: list of tuples - the values of the specified columns
    """
    connection = sqlite3.connect(file)
    cursor = connection.cursor()

    columns = f"{column1}, {column2}" if column2 else  column1
    if indexURL:
        query = f"SELECT {columns} FROM assistance WHERE url = ?"
        cursor.execute(query, (indexURL,))
    else:
        query = f"SELECT {columns} FROM assistance"
        cursor.execute(query)

    lignes = cursor.fetchall()
    connection.close()
    return lignes


def generate_prompt_from_title(title):
    """
    A function that generates a prompt from a title to be used for encoding.
    """
    if ":" in title and len(title.split(' :'))>1:
        produit = title.split(' :')[0]
        service = title.split(' :')[1]
        if  title[-1] != "?":
            return f"Comment peut-on {service} {produit}? Quelles sont les étapes à suivre?"
        else: 
            return f"{service} {produit}."
    else:
        return f"Donnez-moi des informations pour {title}."
    

def load_and_encode_batches(file, column1, column2,batch_size=256): 
    """
    A function that loads the titles and URLs from the database, generates prompts from the titles, and encodes the prompts to embeddings in batches.
    """
    titles = get_TC(file,column1)
    batched_embeddings = []
    for i in range(0, len(titles), batch_size):
        # Generate prompts for each batch of titles
        batch_titles = [titles[j][0] for j in range(i, min(i + batch_size, len(titles)))]
        batch_prompts = [generate_prompt_from_title(title) for title in batch_titles]
        # Encode prompts to embeddings
        batch_embeddings = model.encode(batch_prompts, convert_to_tensor=True)
        batched_embeddings.extend(batch_embeddings.cpu().numpy())  # Convert tensors to numpy and store
    return np.array(batched_embeddings)


def encode_query(question, model):
    """
    A function that encodes a user's question to an embedding.
    """
    question_embedding = model.encode(question, convert_to_tensor=True)
    return question_embedding


def find_most_similar_title(question_embedding, title_embeddings, titles_URLs):
    """
    A fucntion that find the most similar title and return the title and its URL.
    
    Args:
        question_embedding: ndarray - The embedding of the user's question.
        title_embeddings: ndarray - The embeddings of all titles.
        titles_and_URLs: list of tuples - A list where each tuple contains (title, URL).
    
    returns: tuple: The most similar title and its URI.
    """
    similarities = cosine_similarity(question_embedding.reshape(1, -1), title_embeddings)[0]
    most_similar_indexURI = np.argmax(similarities)

    most_similar_title, most_similar_url = titles_URLs[most_similar_indexURI]

    return most_similar_title, most_similar_url


def enhance_response_with_llm(original_response, api_key,query):
    """
    Enhances a response by reformulating it with more details using GPT-3.5-turbo, based on a specific query.

    Args:
        original_response (str): The original response text that needs to be enhanced.
        api_key (str): The API key for authenticating requests to the OpenAI API.
        query (str): The query based on which the original response should be detailed and reformulated.

    Returns:
        str: The enhanced response generated by GPT-3.5-turbo if successful; otherwise, an error message.

    Raises:
        Exception: If there is any issue in the API request or response handling, it prints an error message.
    """

    openai.api_key = api_key  
    prompt =f"Veuillez reformuler cette explication {original_response} avec plus de détails d'après la question {query}"
    try:
        # Create a chat completion request
        chat_completion = openai.ChatCompletion.create(
            model="gpt-3.5-turbo", 
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return chat_completion['choices'][0]['message']['content']
    except Exception as e:

        print(f"An error occurred: {str(e)}")
        return "Error handling your request."

import openai

def main():
    print("Bienvenue à l'Assistant virtuel de Free !")
    print("Tapez 'quitter' pour sortir ou 'aide' pour plus d'informations.")
    print("Assistant : Bonjour! Comment puis-je vous aider aujourd'hui ?") 

    while True:
        input_utilisateur = input("Vous : ")
        #print("Vous : ",input_utilisateur)
        if input_utilisateur.lower() == 'quitter':
            print("Au revoir !")
            break
        elif input_utilisateur.lower() == 'aide':
            print("Tapez votre question ou commande, et je vous fournirai une réponse améliorée basée sur nos données.")
        else:
            # Simuler la récupération d'une réponse et de son URL à partir d'une base de données
            reponse, url = obtenir_reponse_simulee(input_utilisateur)
            print("Assistant :", reponse)
            print("Lien :", url, "\n")
           

def obtenir_reponse_simulee(question_utilisateur):
    """
    Simule la récupération d'une réponse à partir d'une base de données en fonction d'une question de l'utilisateur.
    """
    file = 'assistance.sqlite3'
    column1 = 'title'
    column2 = 'content'
    indexURL = "url"
    #question = "mon numéro de la téléphone fixe est restraint, comment le débloquer?"
    OpenAI_api_key="à compléter"

    title_embeddings = load_and_encode_batches(file, column1, column2)
    question_embedding = encode_query(question_utilisateur, model)
    titles_URLs = get_TC(file, column1, indexURL)  
    title_embeddings = load_and_encode_batches(file, column1, column2)
    similar_title, idxURL = find_most_similar_title(question_embedding, title_embeddings, titles_URLs)
    reponse = get_TC(file,column1,column2,idxURL)[0][1]
    response_enhanced = enhance_response_with_llm(reponse, OpenAI_api_key, question_utilisateur)

    return response_enhanced, idxURL

if __name__ == "__main__":
    main()